{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disciplinary-economics",
   "metadata": {},
   "source": [
    "# Text mining & Search Project\n",
    "\n",
    "### Universit√† degli Studi di Milano-Bicocca  2020/2021\n",
    "\n",
    "**Luzzi Federico** 816753 **Peracchi Marco** 800578"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-peeing",
   "metadata": {},
   "source": [
    "# Text Processing & Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effective-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hundred-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for text tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import  WordPunctTokenizer\n",
    "from nltk.tokenize import  BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "miniature-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for stemming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valued-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for text representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz, load_npz # save and load spase matrix\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument # doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wrapped-recognition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download of necessary contents\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "essential-instruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv(\"data/labeled_data.csv\", sep = ',').drop(\"Unnamed: 0\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latter-genome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @shakiraevanss: Criticize Amanda for saying the n word, sure, but don't make jokes about her sexual assault, don't be trash.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example text\n",
    "text = df[\"tweet\"][19999]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-semester",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "favorite-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower() # Lowering case\n",
    "    remove_url = re.sub(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', ' ', text) # Removing url\n",
    "    remove_retweet = re.sub(r\"@\\w+\", \" \",remove_url) # Removing retweet\n",
    "    remove_retweet = re.sub(r\"&\\w+\", \" \",remove_retweet) # Remove &amp\n",
    "    remove_retweet = re.sub(r\"\\b([!#\\$%&\\\\\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\]\\^_`\\{|\\}\\\"~]+)\\b\", \" \",remove_retweet) # Must check this one\n",
    "    remove_retweet = re.sub(r\"([a-z])\\1{3,}\", r\"\\1\",remove_retweet)\n",
    "    remove_punc = remove_retweet.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
    "    final_text = re.sub(r'\\d+', ' ', remove_punc) # Remove number \n",
    "    final_text = re.sub(r'\\s+', ' ', final_text) # Removing exceeding spaces\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "remarkable-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt criticize amanda for saying the n word sure but dont make jokes about her sexual assault dont be trash'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prep = preprocessing(text)\n",
    "text_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-naples",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "touched-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text_clean, tok = \"tweet\"):\n",
    "    if tok == \"tweet\": # TweetTokenizer\n",
    "        tt = TweetTokenizer()\n",
    "        tokenized_text = tt.tokenize(text_clean)\n",
    "    elif tok == \"wordpunct\": # WordPunctTokenizer\n",
    "        wpt = WordPunctTokenizer()\n",
    "        tokenized_text = wpt.tokenize(text_clean)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "soviet-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'criticize', 'amanda', 'for', 'saying', 'the', 'n', 'word', 'sure', 'but', 'dont', 'make', 'jokes', 'about', 'her', 'sexual', 'assault', 'dont', 'be', 'trash']\n"
     ]
    }
   ],
   "source": [
    "text_tok = tokenization(text_prep)\n",
    "print(text_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-working",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "asian-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_text):\n",
    "    remove_sw = []\n",
    "    for token in tokenized_text:\n",
    "        stop_words.append(\"rt\") # Added a stop words, RT of Retweet\n",
    "        if token.lower() not in stop_words:\n",
    "             remove_sw.append(token)\n",
    "    return remove_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "weird-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['criticize', 'amanda', 'saying', 'n', 'word', 'sure', 'dont', 'make', 'jokes', 'sexual', 'assault', 'dont', 'trash']\n"
     ]
    }
   ],
   "source": [
    "text_sw = remove_stopwords(text_tok)\n",
    "print(text_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-michigan",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "black-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(tokenized_text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fresh-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['critic', 'amanda', 'say', 'n', 'word', 'sure', 'dont', 'make', 'joke', 'sexual', 'assault', 'dont', 'trash']\n"
     ]
    }
   ],
   "source": [
    "print(stemmer(text_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-egypt",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "convertible-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos-tagging (1 document)\n",
    "def pos_tagging(doc_token):\n",
    "    return nltk.pos_tag(doc_token)\n",
    "\n",
    "# convertion of pos tagging\n",
    "def get_wordnet_pos(word_tag):\n",
    "    if word_tag.startswith('J'):\n",
    "        return \"a\"\n",
    "    elif word_tag.startswith('V'):\n",
    "        return \"v\"\n",
    "    elif word_tag.startswith('R'):\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"n\"\n",
    "    \n",
    "# lemmatizer one word \n",
    "def lemmatizer(word):\n",
    "    pos = get_wordnet_pos(word[1])\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return wnl.lemmatize(word[0], pos = pos)\n",
    "\n",
    "# lemmatizer one document\n",
    "def lemmatizer_doc(doc_token):\n",
    "    lemmas = [] \n",
    "    \n",
    "    pos_document = pos_tagging(doc_token) # pos tagging\n",
    "    for token in pos_document:\n",
    "        lemmas.append( lemmatizer(token) ) # lemmatization x word\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "italic-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['criticize', 'amanda', 'say', 'n', 'word', 'sure', 'dont', 'make', 'joke', 'sexual', 'assault', 'dont', 'trash']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer_doc(text_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-bargain",
   "metadata": {},
   "source": [
    "### Processing everything on dataset\n",
    "The following cell applies:\n",
    "\n",
    "1. Preprocessing\n",
    "2. Tokenization\n",
    "3. Stopwords removal\n",
    "4. Lemmatization\n",
    "5. Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "occupational-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up function\n",
    "def processing(text):\n",
    "    text_prep = preprocessing(text)\n",
    "    text_prep = tokenization(text_prep)\n",
    "    text_prep = remove_stopwords(text_prep)\n",
    "    #text_prep = lemmatizer_doc(text_prep)\n",
    "    text_prep = stemmer(text_prep)\n",
    "    text_prep = \" \".join(text_prep)\n",
    "    return text_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "royal-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on all text\n",
    "df[\"tweet_clean\"] = df[\"tweet\"].apply(lambda x : processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "quarterly-excitement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>woman shouldnt complain clean hous man alway t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>boy dat cold tyga dwn bad cuffin dat hoe st place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>dawg ever fuck bitch start cri confus shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>look like tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                         tweet_clean  \n",
       "0  woman shouldnt complain clean hous man alway t...  \n",
       "1  boy dat cold tyga dwn bad cuffin dat hoe st place  \n",
       "2         dawg ever fuck bitch start cri confus shit  \n",
       "3                                   look like tranni  \n",
       "4     shit hear might true might faker bitch told ya  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "subjective-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "df.to_csv(\"data/processed_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-genius",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "common-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load & preprocessing --- DONE\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "if 'processed_data.csv' not in os.listdir('data'):\n",
    "    print(\"!!! ERROR !!!\\n --- Before this, previous cells ---\")\n",
    "else:\n",
    "    df = pd.read_csv(\"data/processed_data.csv\", sep = \",\")\n",
    "    print(\"Load & preprocessing --- DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "systematic-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop tweet list NA\n",
    "df[\"tweet_list\"] = df[\"tweet_clean\"].str.split(\" \").tolist()\n",
    "df.dropna(inplace = True)\n",
    "df = df.reset_index(drop = True)# drop 2 NA because tweet is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "revised-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"tweet_clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-tunisia",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "legendary-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781, 15163)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary = True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Shape\n",
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lucky-efficiency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of presence of a word\n",
    "X.toarray()[1][X.toarray()[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fatty-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save bag of words\n",
    "save_npz('data/representations/bag_of_words.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "attempted-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bag of words\n",
    "bag_of_words = load_npz('data/representations/bag_of_words.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-initial",
   "metadata": {},
   "source": [
    "###  Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fitting-jesus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Example of presence of a word\n",
    "X.toarray()[1][X.toarray()[1] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "located-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save count Vector\n",
    "save_npz('data/representations/count_vector.npz', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-regard",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jewish-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    use_idf=True,\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )\n",
    "tfidf = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "changed-malaysia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781, 3614)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "peaceful-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/representations/tf-idf.npy', tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-capability",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "experienced-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec model --- DONE\n"
     ]
    }
   ],
   "source": [
    "# Generate model of vectorization\n",
    "if 'd2v.model' not in os.listdir('models/doc2vec'):\n",
    "    documents = [TaggedDocument (doc, [i]) for i, doc in enumerate(df[\"tweet_list\"])]\n",
    "    model = Doc2Vec (documents, vector_size=100, window=10, min_count=1)\n",
    "    model.save(\"models/doc2vec/d2v.model\")\n",
    "else:\n",
    "    model = Doc2Vec.load(\"models/doc2vec/d2v.model\")\n",
    "print(\"Doc2Vec model --- DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "exterior-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization --- DONE\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "doc2vec = df[\"tweet_list\"].apply(lambda x: model.infer_vector(x))\n",
    "print(\"Vectorization --- DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "smart-attachment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00231703,  0.00256387, -0.00139071, ...,  0.00306892,\n",
       "       -0.00538458,  0.00151551], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "composed-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/representations/doc2vec.npy', doc2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
